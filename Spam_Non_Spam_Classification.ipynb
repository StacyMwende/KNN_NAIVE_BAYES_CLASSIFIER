{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_Non-Spam_Classification",
      "provenance": [],
      "collapsed_sections": [
        "kLG2VTrnTvYL",
        "XecOwPNorl2W",
        "J4wfHZwQrs-t",
        "a9BPYqunry97",
        "7KMRBJ7zr9HD",
        "zSGyg6kWsBUl",
        "iUNbvIvnT7ep",
        "OI3P3YnHUEBk",
        "ckfufNrcUHeH",
        "6XC_g-zKxe-r",
        "FlBMxEDBUc9B",
        "rF2ABPsHUtbZ",
        "vTbdjSrhVIiT",
        "lQ2G4ZPDVOXE",
        "xrmHVMVsVS--",
        "HPQviDmNtta8",
        "qjFHK1CKty7o",
        "HSsicSdvt4Zs"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StacyMwende/KNN_NAIVE_BAYES_CLASSIFIER/blob/master/Spam_Non_Spam_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLG2VTrnTvYL",
        "colab_type": "text"
      },
      "source": [
        "## 1. Defining the Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XecOwPNorl2W",
        "colab_type": "text"
      },
      "source": [
        "### a) Specifying the Data Analytic Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ozBnKfehSAw",
        "colab_type": "text"
      },
      "source": [
        "> Predict the chance of an email being a spam or a non-spam email "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4wfHZwQrs-t",
        "colab_type": "text"
      },
      "source": [
        "### b) Defining the Metric for Success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF-Rz01VPMLn",
        "colab_type": "text"
      },
      "source": [
        "> This can be used by companies that do advertising in order to know on ow to send the emails and not always fall on the spam which atimes could go unviwed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMRBJ7zr9HD",
        "colab_type": "text"
      },
      "source": [
        "### d) Recording the Experimental Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyHJzWuWTddw",
        "colab_type": "text"
      },
      "source": [
        "For our analysis, i will use CRISP-DM methodology which involves:\n",
        "\n",
        "a) Business Understanding\n",
        "\n",
        "b) Data Understanding\n",
        "\n",
        "c) Data Preparation\n",
        "\n",
        "d) Modeling\n",
        "\n",
        "e) Evaluation\n",
        "\n",
        "f) Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNbvIvnT7ep",
        "colab_type": "text"
      },
      "source": [
        "## 2. Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJn2KjW-WMlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mWZOw-8lr1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "14aeadad-126b-48e9-8a20-29afde4f6da7"
      },
      "source": [
        "# Define the column names\n",
        "spam_columns = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
        "'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
        "'word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses',\n",
        "'word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your',\n",
        "'word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george',\n",
        "'word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data',\n",
        "'word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm',\n",
        "'word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project',\n",
        "'word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(',\n",
        "'char_freq_[','char_freq_exclamation','char_freq_dollar','char_freq_hashtag','capital_run_length_average',\n",
        "'capital_run_length_longest','capital_run_length_total','spam']\n",
        "# \n",
        "df=pd.read_csv(\"spambase.data\", names=spam_columns)\n",
        "df.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_exclamation</th>\n",
              "      <th>char_freq_dollar</th>\n",
              "      <th>char_freq_hashtag</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.00               0.64  ...                       278     1\n",
              "1            0.21               0.28  ...                      1028     1\n",
              "2            0.06               0.00  ...                      2259     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "4            0.00               0.00  ...                       191     1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI3P3YnHUEBk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## 3. Checking the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjSVNwgptHxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c4900de-6b76-4bd8-e63c-7ccb2c7b3445"
      },
      "source": [
        "# Determining the no. of records in our dataset\n",
        "print(df.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4601, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45550LOJo_MG",
        "colab_type": "text"
      },
      "source": [
        "> We have a total of 4601 entries and 58 columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-dW4sQWzbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b399c08-f4f0-402d-b168-f5c5c87c69de"
      },
      "source": [
        "# Checking whether each column has an appropriate datatype\n",
        "df.dtypes"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                float64\n",
              "word_freq_address             float64\n",
              "word_freq_all                 float64\n",
              "word_freq_3d                  float64\n",
              "word_freq_our                 float64\n",
              "word_freq_over                float64\n",
              "word_freq_remove              float64\n",
              "word_freq_internet            float64\n",
              "word_freq_order               float64\n",
              "word_freq_mail                float64\n",
              "word_freq_receive             float64\n",
              "word_freq_will                float64\n",
              "word_freq_people              float64\n",
              "word_freq_report              float64\n",
              "word_freq_addresses           float64\n",
              "word_freq_free                float64\n",
              "word_freq_business            float64\n",
              "word_freq_email               float64\n",
              "word_freq_you                 float64\n",
              "word_freq_credit              float64\n",
              "word_freq_your                float64\n",
              "word_freq_font                float64\n",
              "word_freq_000                 float64\n",
              "word_freq_money               float64\n",
              "word_freq_hp                  float64\n",
              "word_freq_hpl                 float64\n",
              "word_freq_george              float64\n",
              "word_freq_650                 float64\n",
              "word_freq_lab                 float64\n",
              "word_freq_labs                float64\n",
              "word_freq_telnet              float64\n",
              "word_freq_857                 float64\n",
              "word_freq_data                float64\n",
              "word_freq_415                 float64\n",
              "word_freq_85                  float64\n",
              "word_freq_technology          float64\n",
              "word_freq_1999                float64\n",
              "word_freq_parts               float64\n",
              "word_freq_pm                  float64\n",
              "word_freq_direct              float64\n",
              "word_freq_cs                  float64\n",
              "word_freq_meeting             float64\n",
              "word_freq_original            float64\n",
              "word_freq_project             float64\n",
              "word_freq_re                  float64\n",
              "word_freq_edu                 float64\n",
              "word_freq_table               float64\n",
              "word_freq_conference          float64\n",
              "char_freq_;                   float64\n",
              "char_freq_(                   float64\n",
              "char_freq_[                   float64\n",
              "char_freq_exclamation         float64\n",
              "char_freq_dollar              float64\n",
              "char_freq_hashtag             float64\n",
              "capital_run_length_average    float64\n",
              "capital_run_length_longest      int64\n",
              "capital_run_length_total        int64\n",
              "spam                            int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckfufNrcUHeH",
        "colab_type": "text"
      },
      "source": [
        "## 4. External Data Source Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L4sl_0WXlbg",
        "colab_type": "text"
      },
      "source": [
        "Making sure your data matches something outside of the dataset is very important. It allows you to ensure that the measurements are roughly in line with what they should be and it serves as a check on what other things might be wrong in your dataset. External validation can often be as simple as checking your data against a single number, as we will do here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlBMxEDBUc9B",
        "colab_type": "text"
      },
      "source": [
        "## 5. Tidying the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWlukLKUvFQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6195c4f-04c2-4868-f053-7d5e2b13ae86"
      },
      "source": [
        "# Checking for Anomalies\n",
        "#Means checking for duplicates in the dataset\n",
        "# \n",
        "print(df.duplicated().any())\n",
        "\n",
        "# If any, print the sum of duplicates\n",
        "print(df.duplicated().sum())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYgzsFZarc7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7486a75-0e40-4a55-a38b-942d2f34c8dd"
      },
      "source": [
        "# Dealing with duplicates\n",
        "\n",
        "df.drop_duplicates(keep='first', inplace = True)\n",
        "\n",
        "print(df.duplicated().any())\n",
        "\n",
        "# Duplicates have been removed"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvCYb6dgW4yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a46c531-2e76-48e6-e274-1f4f82a5db79"
      },
      "source": [
        "# Identifying the Missing Data\n",
        "df.isnull().sum()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                0\n",
              "word_freq_address             0\n",
              "word_freq_all                 0\n",
              "word_freq_3d                  0\n",
              "word_freq_our                 0\n",
              "word_freq_over                0\n",
              "word_freq_remove              0\n",
              "word_freq_internet            0\n",
              "word_freq_order               0\n",
              "word_freq_mail                0\n",
              "word_freq_receive             0\n",
              "word_freq_will                0\n",
              "word_freq_people              0\n",
              "word_freq_report              0\n",
              "word_freq_addresses           0\n",
              "word_freq_free                0\n",
              "word_freq_business            0\n",
              "word_freq_email               0\n",
              "word_freq_you                 0\n",
              "word_freq_credit              0\n",
              "word_freq_your                0\n",
              "word_freq_font                0\n",
              "word_freq_000                 0\n",
              "word_freq_money               0\n",
              "word_freq_hp                  0\n",
              "word_freq_hpl                 0\n",
              "word_freq_george              0\n",
              "word_freq_650                 0\n",
              "word_freq_lab                 0\n",
              "word_freq_labs                0\n",
              "word_freq_telnet              0\n",
              "word_freq_857                 0\n",
              "word_freq_data                0\n",
              "word_freq_415                 0\n",
              "word_freq_85                  0\n",
              "word_freq_technology          0\n",
              "word_freq_1999                0\n",
              "word_freq_parts               0\n",
              "word_freq_pm                  0\n",
              "word_freq_direct              0\n",
              "word_freq_cs                  0\n",
              "word_freq_meeting             0\n",
              "word_freq_original            0\n",
              "word_freq_project             0\n",
              "word_freq_re                  0\n",
              "word_freq_edu                 0\n",
              "word_freq_table               0\n",
              "word_freq_conference          0\n",
              "char_freq_;                   0\n",
              "char_freq_(                   0\n",
              "char_freq_[                   0\n",
              "char_freq_exclamation         0\n",
              "char_freq_dollar              0\n",
              "char_freq_hashtag             0\n",
              "capital_run_length_average    0\n",
              "capital_run_length_longest    0\n",
              "capital_run_length_total      0\n",
              "spam                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-2E9Xgq8Wf",
        "colab_type": "text"
      },
      "source": [
        "> There are no null values in the various features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTbdjSrhVIiT",
        "colab_type": "text"
      },
      "source": [
        "## 7. Implementing the Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK4DfjvFsamU",
        "colab_type": "text"
      },
      "source": [
        "> Create a baseline model which will be used for comparison between the different model classifiers \n",
        "\n",
        ">> We use logistic Regression model as our baseline model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1i3xyPPtGPj",
        "colab_type": "text"
      },
      "source": [
        "## a) Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLZaRzJXJ3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43f89610-5359-4414-ea73-2ae8de1b682f"
      },
      "source": [
        "# x is the independent variable\n",
        "# y is the target variable(price)\n",
        "# \n",
        "X = df.drop(columns = ['spam'], axis=1)\n",
        "y = df['spam']\n",
        "# \n",
        "# Splitting data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=0)\n",
        "# \n",
        "# Standardising the X_train and the X_test to the same scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# Training the model\n",
        "# \n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "# \n",
        "# Fitting the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logmodel = LogisticRegression()\n",
        "logmodel.fit(X_train,y_train)\n",
        "# \n",
        "# Making predictions using the model above\n",
        "# \n",
        "preds = logmodel.predict(X_test)\n",
        "# \n",
        "# Getting the accuracy of our model\n",
        "from sklearn.metrics import accuracy_score, r2_score,mean_absolute_error, mean_squared_error\n",
        "import scipy.stats as stats\n",
        "print(\"Logistic Reg. model accuracy is\", accuracy_score(y_test,preds))\n",
        " "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Reg. model accuracy is 0.9251781472684085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pbaifa7uHns",
        "colab_type": "text"
      },
      "source": [
        "> Our baseline model accuracy is 93% which means that our model does good predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJwi9YNaun4-",
        "colab_type": "text"
      },
      "source": [
        "## b) Gaussian Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BlM3FHTundX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b856a31c-7fbc-487b-be14-0673f8da3a3d"
      },
      "source": [
        "# Getting the independent and dependent variables\n",
        "# \n",
        "S = df.drop(columns = ['spam'], axis=1)\n",
        "y = df['spam']\n",
        "# \n",
        "# Splitting the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(S,y, test_size=0.20, random_state=0)\n",
        "# \n",
        "# Training our model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train) \n",
        "# \n",
        "# Predicting our test predictors\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predicted = model.predict(X_test)\n",
        "# \n",
        "from sklearn.metrics import accuracy_score, r2_score,mean_absolute_error, mean_squared_error\n",
        "import scipy.stats as stats\n",
        "print(\"Naive bayes model accuracy(80-20) is\", accuracy_score(y_test,predicted))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive bayes model accuracy(80-20) is 0.8206650831353919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KKRstXw_4m",
        "colab_type": "text"
      },
      "source": [
        "**70-30 Data split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1IjO8q2xQV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12912c9f-8530-4c20-9bc1-a7463d861a00"
      },
      "source": [
        "# Splitting the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(S,y, test_size=0.30, random_state=0)\n",
        "# \n",
        "# Training our model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train) \n",
        "# \n",
        "# Predicting our test predictors\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predicted = model.predict(X_test)\n",
        "# \n",
        "from sklearn.metrics import accuracy_score, r2_score,mean_absolute_error, mean_squared_error\n",
        "import scipy.stats as stats\n",
        "print(\"Naive bayes model accuracy(70-30) is\", accuracy_score(y_test,predicted))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive bayes model accuracy(70-30) is 0.833729216152019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1bSAuvyxriP",
        "colab_type": "text"
      },
      "source": [
        "**60-40 Data Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzm0cYCwxlya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b138a87-1ea4-4364-e5c0-47d741438c00"
      },
      "source": [
        "# Splitting the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(S,y, test_size=0.40, random_state=0)\n",
        "# \n",
        "# Training our model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train) \n",
        "# \n",
        "# Predicting our test predictors\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predicted = model.predict(X_test)\n",
        "# \n",
        "from sklearn.metrics import accuracy_score, r2_score,mean_absolute_error, mean_squared_error\n",
        "import scipy.stats as stats\n",
        "print(\"Naive bayes model accuracy(60-40) is\", accuracy_score(y_test,predicted))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive bayes model accuracy(60-40) is 0.8319477434679335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziEwHAOyHGF",
        "colab_type": "text"
      },
      "source": [
        "> Comparing the 3 models accuracy, a 70-30 split brought some improvement to our model but a further 60-40 split reduced the model accuracy comparared to the first 80-20 split.\n",
        "\n",
        ">> This means that the training set should have mode data since the model learns better when given more data to train on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTl_srvbzrMj",
        "colab_type": "text"
      },
      "source": [
        "## 8) Appying LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_sEnUNx0jaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L = df.iloc[:, 0:-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "# \n",
        "# Dividing the data to train and test\n",
        "\n",
        "L_train, L_test, y_train, y_test = train_test_split(L, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Performing feature scaling which is done on the independent variables to normalize the data to a particular range\n",
        "\n",
        "sc = StandardScaler()\n",
        "L_train = sc.fit_transform(L_train)\n",
        "L_test = sc.transform(L_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN7v-Xgx1YV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performing Linear Discriminant Analysis(LDA)\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=1)\n",
        "L_train = lda.fit_transform(L_train, y_train)\n",
        "L_test = lda.transform(L_test)\n",
        "# \n",
        "# Training and Making Predictions\n",
        "# We will use the random forest classifier to evaluate the performance of a PCA-reduced algorithms as shown below\n",
        "# import the random forest library\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(L_train, y_train)\n",
        "y_predd = classifier.predict(L_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-w9OOc2HgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1fe125e9-abd2-420b-85a4-06786a1ed2d6"
      },
      "source": [
        "# Evaluating the Performance\n",
        "# We evaluate our model by getting  a confusion matrix and finding the accuracy of the prediction.\n",
        "# \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_predd)\n",
        "print(conf_matrix )\n",
        "print('Accuracy' + str(accuracy_score(y_test, y_predd)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[453  42]\n",
            " [ 35 312]]\n",
            "Accuracy0.9085510688836105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkQ16JYa4PFL",
        "colab_type": "text"
      },
      "source": [
        "> LDA is a way of improving model performance.\n",
        "\n",
        ">> We observe that the models improved in accuracy.\n",
        "\n",
        "> This means that the model is able to make better predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2G4ZPDVOXE",
        "colab_type": "text"
      },
      "source": [
        "## 8. Challenging the solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWVGKGuiYMWg",
        "colab_type": "text"
      },
      "source": [
        "## a) Applying PCA\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3x3SXZ4XT_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "4bca7564-1efa-4ba2-e87d-96e214b3cb83"
      },
      "source": [
        "#Finding the best principal Components\n",
        "# \n",
        "from sklearn.decomposition import PCA\n",
        "np.random.seed(0)\n",
        "X = df.drop(['spam'], axis=1)\n",
        "train_features = X\n",
        "model = PCA(n_components=5).fit(train_features)\n",
        "# number of components\n",
        "n_pcs= model.components_.shape[0]\n",
        "# \n",
        "# get the index of the most important feature on EACH component\n",
        "most_import = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
        "initial_feature_names = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
        "'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
        "'word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses',\n",
        "'word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your',\n",
        "'word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george',\n",
        "'word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data',\n",
        "'word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm',\n",
        "'word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project',\n",
        "'word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(',\n",
        "'char_freq_[','char_freq_exclamation','char_freq_dollar','char_freq_hashtag','capital_run_length_average',\n",
        "'capital_run_length_longest','capital_run_length_total']\n",
        "# get the names\n",
        "most_import_names = [initial_feature_names[most_import[i]] for i in range(n_pcs)]\n",
        "# \n",
        "dic = {'PC{}'.format(i): most_import_names[i] for i in range(n_pcs)}\n",
        "# build the dataframe\n",
        "df_pca = pd.DataFrame(dic.items())\n",
        "df_pca"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PC0</td>\n",
              "      <td>capital_run_length_total</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PC1</td>\n",
              "      <td>capital_run_length_longest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PC2</td>\n",
              "      <td>capital_run_length_average</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PC3</td>\n",
              "      <td>word_freq_you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PC4</td>\n",
              "      <td>word_freq_george</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0                           1\n",
              "0  PC0    capital_run_length_total\n",
              "1  PC1  capital_run_length_longest\n",
              "2  PC2  capital_run_length_average\n",
              "3  PC3               word_freq_you\n",
              "4  PC4            word_freq_george"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4sM2skX9gW0",
        "colab_type": "text"
      },
      "source": [
        ">  The whole dataset of 58 columns ahs been reduced to 5 components from the one causing more variance to the least in variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sVRm5_-96y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the independent and dependent variables\n",
        "# \n",
        "pca = PCA(n_components = 5)\n",
        "pca.fit(X)\n",
        "x_pca = pca.transform(X)\n",
        "\n",
        "# Splitting the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.20)\n",
        "# \n",
        "pca = PCA()\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkD0HJkmF_Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and Making Predictions\n",
        "# \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred_pca = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjYUbFT2GJD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1c7ae004-2196-4783-cfb3-bdaa0fb62b0b"
      },
      "source": [
        "# Step 10: Performance Evaluation\n",
        "# \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_pca)\n",
        "print(cm)\n",
        "print('Accuracy' , accuracy_score(y_test, y_pred_pca))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[463  62]\n",
            " [ 80 237]]\n",
            "Accuracy 0.831353919239905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nle297yuGWRw",
        "colab_type": "text"
      },
      "source": [
        "> Comparing to the LDA accuracy, the model does not give a better score even after reducing the components.\n",
        "\n",
        "> This could mean that better methods should be used to classify an email as either spam or non-spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrmHVMVsVS--",
        "colab_type": "text"
      },
      "source": [
        "## 9. Follow up questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pth2qSWhuBIy",
        "colab_type": "text"
      },
      "source": [
        "> At this point, we can refine our question or collect new data, all in an iterative process to get at the truth.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPQviDmNtta8",
        "colab_type": "text"
      },
      "source": [
        "### a). Did we have the right data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vd2mEWSG2Nd",
        "colab_type": "text"
      },
      "source": [
        "> The data was good though more column explanation should be done for better understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjFHK1CKty7o",
        "colab_type": "text"
      },
      "source": [
        "### b). Do we need other data to answer our question?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAu91JTrG_GH",
        "colab_type": "text"
      },
      "source": [
        "> Accordinfg to the model performance, the features performed well meaning the data provided was fit to answer the question in place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSsicSdvt4Zs",
        "colab_type": "text"
      },
      "source": [
        "### c). Did we have the right question?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b8Jt_ijHUam",
        "colab_type": "text"
      },
      "source": [
        "> The question provided was good."
      ]
    }
  ]
}